{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\"\"\"\n",
    "Seguimiento de loss, accuracy y imprimir graficos.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "arr_loss_train = []\n",
    "arr_loss_test = []\n",
    "arr_acc_train = []\n",
    "arr_acc_test = []\n",
    "def loss_plot(epochs, loss):\n",
    "    plt.plot([*range(epochs)], loss)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "def acc_plot(epochs,acc):\n",
    "    plt.plot([*range(epochs)], acc)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Algoritmo de entrenamiento y de validacion de accuraccy.\n",
    "\"\"\"\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    print_every_x_batches = 100\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # calculate accuracy\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # print loss during training, not recorded.\n",
    "        if batch % print_every_x_batches == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    arr_loss_train.append(loss.cpu().detach().numpy().item())\n",
    "    correct /= size\n",
    "    arr_acc_train.append(correct)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    model.eval() # pone a la red en moodo evaluacion, desactiva capas dropout.\n",
    "    with torch.no_grad(): # desactiva el proceso de calculo y guardado de valores intermedios\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    arr_loss_test.append(test_loss)\n",
    "    arr_acc_test.append(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instanciar modelo a ajustar hiperparametros.\n",
    "\"\"\"\n",
    "from modelo_convolucional import RedConvolucional\n",
    "model = RedConvolucional()\n",
    "\n",
    "\"\"\"\n",
    "Definir funcion de costo y optimizador\n",
    "Explicacion sobre la optimizacion \n",
    "https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "# CrossEntropyLoss requiere el output \"logits\", no es necesario pasarlo por el softmax, ya que lo calcula dentro. https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# se necesita enviar los parametros del modelo al optimizador para que los pueda actualizar.\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aca abajo, lo importante para la optimizacion de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "== Status ==\n",
      "Current time: 2022-11-13 00:00:37 (running for 00:00:00.13)\n",
      "Memory usage on this node: 7.0/15.4 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
      "Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/5.79 GiB heap, 0.0/2.9 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/braian/ray_results/train_cifar_2022-11-13_00-00-37\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------+----------+---------------------+--------------+-----------------+----------------------+------------------------+----------------------+------------------------+-----------+-----------+\n",
      "| Trial name              | status   | loc                 |   batch_size |   learning_rate |   cant_filtros_conv1 |   kernel_size_maxpool1 |   cant_filtros_conv2 |   kernel_size_maxpool2 |   full_l1 |   full_l2 |\n",
      "|-------------------------+----------+---------------------+--------------+-----------------+----------------------+------------------------+----------------------+------------------------+-----------+-----------|\n",
      "| train_cifar_599f1_00000 | RUNNING  | 192.168.0.187:22295 |            8 |       0.0019577 |                   18 |                      3 |                   16 |                      3 |       140 |       104 |\n",
      "+-------------------------+----------+---------------------+--------------+-----------------+----------------------+------------------------+----------------------+------------------------+-----------+-----------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/braian/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py\", line 853, in _wait_and_handle_event\n    self._on_training_result(\n  File \"/home/braian/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py\", line 978, in _on_training_result\n    self._process_trial_results(trial, result)\n  File \"/home/braian/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py\", line 1061, in _process_trial_results\n    decision = self._process_trial_result(trial, result)\n  File \"/home/braian/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py\", line 1098, in _process_trial_result\n    self._validate_result_metrics(flat_result)\n  File \"/home/braian/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py\", line 1194, in _validate_result_metrics\n    raise ValueError(\nValueError: Trial returned a result which did not include the specified metric(s) `loss` that `AsyncHyperBandScheduler` expects. Make sure your calls to `tune.report()` include the metric, or set the TUNE_DISABLE_STRICT_METRIC_CHECKING environment variable to 1. Result: {'trial_id': '599f1_00000', 'experiment_id': '14f2e30a2234411d83c3bfab52d52225', 'date': '2022-11-13_00-00-38', 'timestamp': 1668308438, 'pid': 22295, 'hostname': 'braian-pc-linux', 'node_ip': '192.168.0.187', 'done': True, 'config/batch_size': 8, 'config/learning_rate': 0.0019577033484982163, 'config/cant_filtros_conv1': 18, 'config/kernel_size_maxpool1': 3, 'config/cant_filtros_conv2': 16, 'config/kernel_size_maxpool2': 3, 'config/full_l1': 140, 'config/full_l2': 104}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py:853\u001b[0m, in \u001b[0;36mTrialRunner._wait_and_handle_event\u001b[0;34m(self, next_trial)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39mif\u001b[39;00m event\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m _ExecutorEventType\u001b[39m.\u001b[39mTRAINING_RESULT:\n\u001b[0;32m--> 853\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_training_result(\n\u001b[1;32m    854\u001b[0m         trial, result[_ExecutorEvent\u001b[39m.\u001b[39;49mKEY_FUTURE_RESULT]\n\u001b[1;32m    855\u001b[0m     )\n\u001b[1;32m    856\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py:978\u001b[0m, in \u001b[0;36mTrialRunner._on_training_result\u001b[0;34m(self, trial, result)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[39mwith\u001b[39;00m warn_if_slow(\u001b[39m\"\u001b[39m\u001b[39mprocess_trial_result\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 978\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_trial_results(trial, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py:1061\u001b[0m, in \u001b[0;36mTrialRunner._process_trial_results\u001b[0;34m(self, trial, results)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[39mwith\u001b[39;00m warn_if_slow(\u001b[39m\"\u001b[39m\u001b[39mprocess_trial_result\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1061\u001b[0m     decision \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_trial_result(trial, result)\n\u001b[1;32m   1062\u001b[0m \u001b[39mif\u001b[39;00m decision \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1063\u001b[0m     \u001b[39m# If we didn't get a decision, this means a\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[39m# non-training future (e.g. a save) was scheduled.\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m     \u001b[39m# We do not allow processing more results then.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py:1098\u001b[0m, in \u001b[0;36mTrialRunner._process_trial_result\u001b[0;34m(self, trial, result)\u001b[0m\n\u001b[1;32m   1097\u001b[0m flat_result \u001b[39m=\u001b[39m flatten_dict(result)\n\u001b[0;32m-> 1098\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_result_metrics(flat_result)\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopper(trial\u001b[39m.\u001b[39mtrial_id, result) \u001b[39mor\u001b[39;00m trial\u001b[39m.\u001b[39mshould_stop(flat_result):\n",
      "File \u001b[0;32m~/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py:1194\u001b[0m, in \u001b[0;36mTrialRunner._validate_result_metrics\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[39mif\u001b[39;00m report_metric:\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrial returned a result which did not include the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1196\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mspecified metric(s) `\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m` that `\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m` expects. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1197\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMake sure your calls to `tune.report()` include the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmetric, or set the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1199\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTUNE_DISABLE_STRICT_METRIC_CHECKING \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1200\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39menvironment variable to 1. Result: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1201\u001b[0m             report_metric, location, result\n\u001b[1;32m   1202\u001b[0m         )\n\u001b[1;32m   1203\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Trial returned a result which did not include the specified metric(s) `loss` that `AsyncHyperBandScheduler` expects. Make sure your calls to `tune.report()` include the metric, or set the TUNE_DISABLE_STRICT_METRIC_CHECKING environment variable to 1. Result: {'trial_id': '599f1_00000', 'experiment_id': '14f2e30a2234411d83c3bfab52d52225', 'date': '2022-11-13_00-00-38', 'timestamp': 1668308438, 'pid': 22295, 'hostname': 'braian-pc-linux', 'node_ip': '192.168.0.187', 'done': True, 'config/batch_size': 8, 'config/learning_rate': 0.0019577033484982163, 'config/cant_filtros_conv1': 18, 'config/kernel_size_maxpool1': 3, 'config/cant_filtros_conv2': 16, 'config/kernel_size_maxpool2': 3, 'config/full_l1': 140, 'config/full_l2': 104}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39mprint\u001b[39m(data_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39m# You can change the number of GPUs per trial here:\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     main(num_samples\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, max_num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, gpus_per_trial\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;32m/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb Cell 6\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m scheduler \u001b[39m=\u001b[39m ASHAScheduler(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     max_t\u001b[39m=\u001b[39mmax_num_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     grace_period\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     reduction_factor\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m reporter \u001b[39m=\u001b[39m CLIReporter(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     parameter_columns\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(config\u001b[39m.\u001b[39mkeys()),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     metric_columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtraining_iteration\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m result \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# partial es una funcion de orden superior de tipo curry\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     partial(train_cifar, data_dir\u001b[39m=\u001b[39;49mdata_dir),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     resources_per_trial\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m2\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m\"\u001b[39;49m: gpus_per_trial},\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     num_samples\u001b[39m=\u001b[39;49mnum_samples,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     progress_reporter\u001b[39m=\u001b[39;49mreporter)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m best_trial \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mget_best_trial(\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlast\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/braian/Documents/tesis-red-neuronal/ajustar.ipynb#W5sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest trial config: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(best_trial\u001b[39m.\u001b[39mconfig))\n",
      "File \u001b[0;32m~/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/tune.py:741\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote)\u001b[0m\n\u001b[1;32m    734\u001b[0m progress_reporter\u001b[39m.\u001b[39msetup(\n\u001b[1;32m    735\u001b[0m     start_time\u001b[39m=\u001b[39mtune_start,\n\u001b[1;32m    736\u001b[0m     total_samples\u001b[39m=\u001b[39msearch_alg\u001b[39m.\u001b[39mtotal_samples,\n\u001b[1;32m    737\u001b[0m     metric\u001b[39m=\u001b[39mmetric,\n\u001b[1;32m    738\u001b[0m     mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    739\u001b[0m )\n\u001b[1;32m    740\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m runner\u001b[39m.\u001b[39mis_finished() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39msignal\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 741\u001b[0m     runner\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    742\u001b[0m     \u001b[39mif\u001b[39;00m has_verbosity(Verbosity\u001b[39m.\u001b[39mV1_EXPERIMENT):\n\u001b[1;32m    743\u001b[0m         _report_progress(runner, progress_reporter)\n",
      "File \u001b[0;32m~/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py:886\u001b[0m, in \u001b[0;36mTrialRunner.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[39mif\u001b[39;00m next_trial:\n\u001b[1;32m    884\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot new trial to run: \u001b[39m\u001b[39m{\u001b[39;00mnext_trial\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 886\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_and_handle_event(next_trial)\n\u001b[1;32m    888\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_experiment_if_needed()\n\u001b[1;32m    890\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py:865\u001b[0m, in \u001b[0;36mTrialRunner._wait_and_handle_event\u001b[0;34m(self, next_trial)\u001b[0m\n\u001b[1;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    864\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 865\u001b[0m     \u001b[39mraise\u001b[39;00m TuneError(traceback\u001b[39m.\u001b[39mformat_exc())\n",
      "\u001b[0;31mTuneError\u001b[0m: Traceback (most recent call last):\n  File \"/home/braian/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py\", line 853, in _wait_and_handle_event\n    self._on_training_result(\n  File \"/home/braian/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py\", line 978, in _on_training_result\n    self._process_trial_results(trial, result)\n  File \"/home/braian/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py\", line 1061, in _process_trial_results\n    decision = self._process_trial_result(trial, result)\n  File \"/home/braian/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py\", line 1098, in _process_trial_result\n    self._validate_result_metrics(flat_result)\n  File \"/home/braian/miniconda3/envs/tesis/lib/python3.9/site-packages/ray/tune/execution/trial_runner.py\", line 1194, in _validate_result_metrics\n    raise ValueError(\nValueError: Trial returned a result which did not include the specified metric(s) `loss` that `AsyncHyperBandScheduler` expects. Make sure your calls to `tune.report()` include the metric, or set the TUNE_DISABLE_STRICT_METRIC_CHECKING environment variable to 1. Result: {'trial_id': '599f1_00000', 'experiment_id': '14f2e30a2234411d83c3bfab52d52225', 'date': '2022-11-13_00-00-38', 'timestamp': 1668308438, 'pid': 22295, 'hostname': 'braian-pc-linux', 'node_ip': '192.168.0.187', 'done': True, 'config/batch_size': 8, 'config/learning_rate': 0.0019577033484982163, 'config/cant_filtros_conv1': 18, 'config/kernel_size_maxpool1': 3, 'config/cant_filtros_conv2': 16, 'config/kernel_size_maxpool2': 3, 'config/full_l1': 140, 'config/full_l2': 104}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "importar funciones utilidad.\n",
    "\"\"\"\n",
    "from cargar_datos import cargar_datasets, cargar_dataloaders, classes\n",
    "from functools import partial\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=0.5):\n",
    "    data_dir = os.path.abspath(\"./data\")\n",
    "    # se carga una vez, para que si no esta descargado, se descarge y se valide, el resto de veces en las pruebas, se saltara esta parte.\n",
    "    cargar_datasets(data_dir)\n",
    "    \"\"\"\n",
    "    variable de configuracion, Ray tune modificara sus valores en el proceso de ajuste de hiperparametros.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"batch_size\": tune.choice([8, 16, 32]),\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n",
    "        # red convolucional\n",
    "        \"cant_filtros_conv1\": tune.choice([6, 12, 18]),\n",
    "        \"kernel_size_maxpool1\": tune.choice([2, 3]),\n",
    "        \"cant_filtros_conv2\": tune.choice([16, 22, 28]),\n",
    "        \"kernel_size_maxpool2\": tune.choice([2, 3]),\n",
    "        \"full_l1\": tune.choice([120, 140, 160]),\n",
    "        \"full_l2\": tune.choice([84, 104, 124])\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=list(config.keys()),\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    result = tune.run(\n",
    "        # partial es una funcion de orden superior de tipo curry\n",
    "        partial(train_cifar, data_dir=data_dir),\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter)\n",
    "\n",
    "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(\n",
    "        best_trial.last_result[\"loss\"]))\n",
    "    print(\"Best trial final validation accuracy: {}\".format(\n",
    "        best_trial.last_result[\"accuracy\"]))\n",
    "\n",
    "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if gpus_per_trial > 1:\n",
    "            best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.value\n",
    "    model_state, optimizer_state = torch.load(os.path.join(\n",
    "        best_checkpoint_dir, \"checkpoint\"))\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_acc = test_accuracy(best_trained_model, device)\n",
    "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
    "\n",
    "\n",
    "def train_cifar(config,data_dir = \"pepe\"):\n",
    "    print(config)\n",
    "    print(data_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of GPUs per trial here:\n",
    "    main(num_samples=1, max_num_epochs=10, gpus_per_trial=0)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9abe774f74fe6e9a34c044080c00539c2573e8d4e4c28ec478136b0c7aa9cb53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
