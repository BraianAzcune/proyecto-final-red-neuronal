{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n",
      "Total Memory:  23.7 GB\n",
      "Memory Usage:\n",
      "Allocated: 0.3 GB\n",
      "Cached:    0.9 GB\n"
     ]
    }
   ],
   "source": [
    "# Memoria GPU\n",
    "import torch\n",
    "#x = torch.rand(100,3,512,512).cuda() # simulacion de 100 imagenes de 512x512 a color...\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\"Total Memory: \",round(torch.cuda.mem_get_info(0)[1]/1024**3,1),'GB')\n",
    "print('Memory Usage:')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinar de forma general un batch size apropiado para nuestro modelo.\n",
    "\n",
    "https://discuss.pytorch.org/t/how-to-determine-the-largest-batch-size-of-a-given-model-saturating-the-gpu/146075\n",
    "\n",
    "Aplicando una busqueda binaria con la funcion de Suraj.\n",
    "\n",
    "En principio siempre se quiere conseguir el tamaño maximo de batch size que soporte el hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "dimension_imagen = [3,32,32]\n",
    "\n",
    "def proc_time(b_sz, model, n_iter=10):\n",
    "    x = torch.rand(b_sz, *dimension_imagen).cuda()\n",
    "    model.cuda()\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(n_iter):\n",
    "        model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time() - start\n",
    "    throughput = b_sz * n_iter / end\n",
    "    print(f\"Batch: {b_sz} \\t {throughput} samples/sec\")\n",
    "    return (b_sz, throughput, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 \t 10919.822962770111 samples/sec\n",
      "Batch: 6 \t 20286.839177750906 samples/sec\n",
      "Batch: 11 \t 95305.39971080355 samples/sec\n",
      "Batch: 16 \t 161591.29304117506 samples/sec\n",
      "Batch: 21 \t 242378.60209135938 samples/sec\n",
      "Batch: 26 \t 198132.09302325582 samples/sec\n",
      "Batch: 31 \t 234234.2352729238 samples/sec\n",
      "Batch: 36 \t 256663.17185109638 samples/sec\n",
      "Batch: 41 \t 185909.6908108108 samples/sec\n",
      "Batch: 46 \t 252173.55116978174 samples/sec\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,51,5):\n",
    "    proc_time(i,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/46654424/how-to-calculate-optimal-batch-size \n",
    "\n",
    "Use the summaries provided by pytorchsummary (pip install) or keras (builtin).\n",
    "\n",
    "E.g.\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model)\n",
    ".....\n",
    ".....\n",
    "================================================================\n",
    "Total params: 1,127,495\n",
    "Trainable params: 1,127,495\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.02\n",
    "Forward/backward pass size (MB): 13.93\n",
    "Params size (MB): 4.30\n",
    "Estimated Total Size (MB): 18.25\n",
    "----------------------------------------------------------------\n",
    "Each instance you put in the batch will require a full forward/backward pass in memory, your model you only need once. People seem to prefer batch sizes of powers of two, probably because of automatic layout optimization on the gpu.\n",
    "\n",
    "Don't forget to linearly increase your learning rate when increasing the batch size.\n",
    "\n",
    "Let's assume we have a Tesla P100 at hand with 16 GB memory.\n",
    "\n",
    "(16000 - model_size) / (forward_back_ward_size)\n",
    "(16000 - 4.3) / 18.25 = 1148.29\n",
    "rounded to powers of 2 results in batch size 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Sequential                               [10, 64, 28, 28]          --\n",
      "├─Conv2d: 1-1                            [10, 20, 30, 30]          560\n",
      "├─ReLU: 1-2                              [10, 20, 30, 30]          --\n",
      "├─Conv2d: 1-3                            [10, 64, 28, 28]          11,584\n",
      "├─ReLU: 1-4                              [10, 64, 28, 28]          --\n",
      "==========================================================================================\n",
      "Total params: 12,144\n",
      "Trainable params: 12,144\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 95.86\n",
      "==========================================================================================\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 5.45\n",
      "Params size (MB): 0.05\n",
      "Estimated Total Size (MB): 5.63\n",
      "==========================================================================================\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "          nn.Conv2d(3,20,3),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,3),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "batch_size = 10\n",
    "resumen = summary(model, input_size=(batch_size, 3, 32, 32))\n",
    "print(resumen)\n",
    "\n",
    "import re\n",
    "model_size = re.search(r\"Params size (MB):\",resumen.__repr__())\n",
    "print(model_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MONITORIZAR MEMORIA DE LA GPU EN ENTRENAMIENTOS\n",
    "https://wandb.ai/wandb/common-ml-errors/reports/How-To-Use-GPU-with-PyTorch---VmlldzozMzAxMDk "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9abe774f74fe6e9a34c044080c00539c2573e8d4e4c28ec478136b0c7aa9cb53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
